# -*- coding: utf-8 -*-
"""Nlp_HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6TgHGg3pvFGc3wmQrGKrvT4pFEuNQzp
"""

#1-2
import nltk

#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')

sentence = "Natural language processing is fun! This text is a sample text."

# Word Tokenization
tokens = nltk.word_tokenize(sentence)
print("Word Tokens:")
print(tokens)

# POS Tagging
pos_tags = nltk.pos_tag(tokens)
print("\nPOS Tags:")
print(pos_tags)
#----------------------------------------------------------------------------------------------

#1-3
from nltk.chunk import RegexpParser

# Noun Phrase Chunking
grammar = r"""
  NP: {<DT>?<JJ>*<NN>}  # Chunk determiner, adjectives, and nouns
      {<NNP>+}          # Chunk consecutive proper nouns
"""
chunk_parser = RegexpParser(grammar)
chunks = chunk_parser.parse(pos_tags)


# Print Noun Phrase Chunks
print("\nNoun Phrase Chunks:")
for subtree in chunks.subtrees():
    if subtree.label() == 'NP':
        print(subtree)
#----------------------------------------------------------------------------------------------

#1-4
from nltk.tree import Tree



# Print Noun Phrase Chunks as Pretty-Printed Tree
print("\nNoun Phrase Chunks:")
for subtree in chunks.subtrees():
    if subtree.label() == 'NP':
        print(subtree.pretty_print())
#----------------------------------------------------------------------------------------------

#1-5
import nltk
from nltk.chunk import RegexpParser
from nltk.tree import Tree

sentence = "She decided to take a stroll in the park."

# Word Tokenization
tokens = nltk.word_tokenize(sentence)

# POS Tagging
pos_tags = nltk.pos_tag(tokens)

# Verb Phrase Chunking
grammar = r"""
  VP: {<VB.*><TO>?<VB.*>?}  # Chunk verbs and optional "to" followed by verbs
"""
chunk_parser = RegexpParser(grammar)
chunks = chunk_parser.parse(pos_tags)

# Print Word Tokens
print("Word Tokens:")
print(tokens)

# Print POS Tags
print("\nPOS Tags:")
print(pos_tags)


# Print Verb Phrase Chunks as Pretty-Printed Tree
print("\nNoun Phrase Chunks:")
for subtree in chunks.subtrees():
    if subtree.label() == 'VP':
        print(subtree.pretty_print())

#----------------------------------------------------------------------------------------------

#2-1
import re

# Define the regular expression patterns for entity recognition
entity_patterns = [
    (re.compile(r'\b(rock)\b', re.IGNORECASE), 'B-ENTITY'),
    (re.compile(r'\b(guitar)\b', re.IGNORECASE), 'B-ENTITY'),
    (re.compile(r'\b(music)\b', re.IGNORECASE), 'B-ENTITY'),
]

# Read the "Rock.txt" dataset
with open('Rock.txt', 'r') as file:
    text = file.read()

# Apply IOB encoding to the dataset
encoded_text = []
words = text.split()

for word in words:
    tagged = False
    for pattern, tag in entity_patterns:
        if pattern.match(word):
            encoded_text.append((word, tag))
            tagged = True
            break
    if not tagged:
        encoded_text.append((word, 'O'))

# Display the IOB encoded result
for word, tag in encoded_text:
    print(f'{word}\t{tag}')
#----------------------------------------------------------------------------------------------

#2-2
import nltk
from nltk.tag import StanfordNERTagger

# Path to the Stanford NER model and JAR file
stanford_ner_model = '/content/english.muc.7class.distsim.crf.ser.gz'
stanford_ner_jar = '/content/stanford-ner.jar'

# Path to the "Rock.txt" dataset
rock_dataset = 'Rock.txt'

# Initialize the Stanford NER tagger
ner_tagger = StanfordNERTagger(stanford_ner_model, stanford_ner_jar)

# Read the "Rock.txt" dataset
with open(rock_dataset, 'r') as file:
    text = file.read()

# Tokenize the text into sentences
sentences = nltk.sent_tokenize(text)

# Extract named entities from each sentence
named_entities = []
for sentence in sentences:
    tokens = nltk.word_tokenize(sentence)
    tagged_entities = ner_tagger.tag(tokens)
    named_entities.extend([(token, tag) for token, tag in tagged_entities if tag != 'O'])

# Print the names of people and places
people = set()
places = set()
for token, tag in named_entities:
    if tag == 'PERSON':
        people.add(token)
    elif tag == 'LOCATION':
        places.add(token)

print("People:")
for person in people:
    print(person)

print("\nPlaces:")
for place in places:
    print(place)
#----------------------------------------------------------------------------------------------

#2-3
import spacy

# Load the pre-trained English language model
nlp = spacy.load("en_core_web_sm")

# Path to the "Rock.txt" dataset
rock_dataset = "Rock.txt"

# Read the "Rock.txt" dataset
with open(rock_dataset, "r") as file:
    text = file.read()

# Process the text with Spacy
doc = nlp(text)

# Extract named entities and print the names of people and places
people = set()
places = set()

for entity in doc.ents:
    if entity.label_ == "PERSON":
        people.add(entity.text)
    elif entity.label_ == "GPE":
        places.add(entity.text)

print("People:")
for person in people:
    print(person)

print("\nPlaces:")
for place in places:
    print(place)

#3-1/3-2
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Path to the "Rock.txt" dataset
rock_dataset = "Rock.txt"

# Read the "Rock.txt" dataset
with open(rock_dataset, "r") as file:
    text = file.read()

# Tokenize the text into words
tokens = word_tokenize(text)

# Remove stopwords and punctuation marks
stopwords_set = set(stopwords.words("english"))
punctuation_set = set(string.punctuation)
filtered_tokens = [
    token.lower()
    for token in tokens
    if token.lower() not in stopwords_set and token not in punctuation_set
]

# Lemmatize the filtered tokens
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

# Convert the pre-processed tokens back to a string
preprocessed_text = " ".join(lemmatized_tokens)

# Apply tf-idf vectorization
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([preprocessed_text])

# Get the feature names (keywords)
feature_names = vectorizer.get_feature_names_out()

# Get the tf-idf scores for each feature
tfidf_scores = tfidf_matrix.toarray()[0]

# Sort the feature names based on the tf-idf scores in descending order
sorted_indices = tfidf_scores.argsort()[::-1]

# Extract the top ten most important keywords
top_keywords = [feature_names[idx] for idx in sorted_indices[:10]]

# Print the result
print("Top Keywords:")
for keyword in top_keywords:
    print(keyword)
#----------------------------------------------------------------------------------------------

#3-3
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity



preprocessed_sentences = []
for sentence in sentences:
    # Tokenize the sentence into words
    tokens = word_tokenize(sentence)
    # Remove stopwords and punctuation marks
    filtered_tokens = [
        token.lower()
        for token in tokens
        if token.lower() not in stopwords_set and token not in punctuation_set
    ]
    # Lemmatize the tokens
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    # Convert the preprocessed tokens back to a string
    preprocessed_sentence = " ".join(lemmatized_tokens)
    preprocessed_sentences.append(preprocessed_sentence)

# Apply tf-idf vectorization
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)

# Get the feature names (keywords)
feature_names = vectorizer.get_feature_names_out()

# Get the tf-idf scores for each feature
tfidf_scores = tfidf_matrix.toarray()

# Calculate the sentence scores based on the tf-idf scores
sentence_scores = tfidf_scores.sum(axis=1)

# Sort the sentences based on the scores in descending order
sorted_indices = sentence_scores.argsort()[::-1]
top_sentences = [sentences[idx] for idx in sorted_indices[:5]]

# Print the summarized text
print("Summarized Text:")
for sentence in top_sentences:
    print(sentence)
#----------------------------------------------------------------------------------------------

#3-5
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from yake import KeywordExtractor
from sumy.parsers.plaintext import PlaintextParser #!pip install yake setuptools !pip install sumy setuptools
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer


# Read the 'Rock.txt' file
with open('Rock.txt', 'r') as file:
    text = file.read()

# Tokenize the text into words
tokens = word_tokenize(text.lower())

# Remove stopwords and punctuation marks
stop_words = set(stopwords.words('english'))
punctuation = ['.', ',', ';', ':', '!', '?', '(', ')', '[', ']', '{', '}', "'", '"']
filtered_tokens = [token for token in tokens if token not in stop_words and token not in punctuation]

# Lemmatize the tokens
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

# Convert the lemmatized tokens back to text
preprocessed_text = ' '.join(lemmatized_tokens)

# Extract keywords using YAKE
keyword_extractor = KeywordExtractor()
keywords = keyword_extractor.extract_keywords(preprocessed_text)

# Get the top 10 keywords
important_keywords = [keyword[0] for keyword in keywords][:10]

# Summarize the text using TextRank algorithm
parser = PlaintextParser.from_string(text, Tokenizer("english"))
summarizer = TextRankSummarizer()
summary = summarizer(parser.document, 5)  # Get 5 sentences for the summary

# Print the keywords and summary
print("Keywords:")
for keyword in important_keywords:
    print(keyword)

print("\nSummary:")
for sentence in summary:
    print(sentence)
#----------------------------------------------------------------------------------------------

#4
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
df = pd.read_csv('tmdb_5000_movies.csv')

# Preprocess the genres and keywords columns
df['genres'] = df['genres'].apply(lambda x: ' '.join([genre['name'] for genre in eval(x)]))
df['keywords'] = df['keywords'].apply(lambda x: ' '.join([keyword['name'] for keyword in eval(x)]))

# Concatenate genres and keywords into a single feature
df['features'] = df['genres'] + ' ' + df['keywords']

# Apply TF-IDF vectorization to the features
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['features'])

# Calculate the cosine similarity matrix
cosine_sim = cosine_similarity(tfidf_matrix)

# Function to get top similar movies
def get_similar_movies(movie_title, top_n=5):
    # Find the index of the searched movie
    movie_index = df[df['title'] == movie_title].index[0]

    # Get the similarity scores of the searched movie with all other movies
    similarity_scores = list(enumerate(cosine_sim[movie_index]))

    # Sort the movies based on the similarity scores
    sorted_movies = sorted(similarity_scores, key=lambda x: x[1], reverse=True)

    # Get the top similar movies (excluding the searched movie itself)
    top_similar_movies = [(df.iloc[movie[0]]['title'], movie[1]) for movie in sorted_movies if movie[0] != movie_index][:top_n]

    return top_similar_movies

# Searched movies and their recommendations
searched_movies = ['Flywheel', 'Mortal Kombat', 'Frozen']

for movie in searched_movies:
    recommendations = get_similar_movies(movie)
    print(f"Recommended movies for '{movie}':")
    for recommendation in recommendations:
        print(f"{recommendation[0]} (Similarity: {recommendation[1]})")
    print()