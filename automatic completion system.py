# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OjsI-9k4axswolYuLR4h4VjAIfOT4t0v
"""

from hazm import *
import pandas as pd

df = pd.read_csv('digikala_comment.csv')
comments = df['comment']

# Remove whitespace
def remove(string):
    return string.replace(" ", "")

comment_list = []
for comment in comments:
    sentences = str(comment).strip().split('.') # level 1-1 of preprocessing: Break down each comment into its sentences
    for sen in sentences:
        remove(sen) # level 1-2 of preprocessing: remove extra spaces
        sen = ''.join(sen)
        comment_list.append(sen)

from hazm import word_tokenize,stopwords_list

temp_list = []
for comment in comment_list:
    words = word_tokenize(comment) # level 1-3 of preprocessing: Text tokenization
    filtered_words = [word for word in words if word not in stopwords_list()] # level 1-3 of preprocessing: Remove stopwords
    filtered_text = ' '.join(filtered_words) # Convert to new text
    temp_list.append(filtered_text)

from string import punctuation
import re


def remove_punctuation(text):
    # define the pattern of punctuations to remove
    pattern = re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)
    # use regex to substitute the pattern with an empty string
    return pattern

unpunc_text =  remove_punctuation(str(temp_list)) # level 1-3 of preprocessing: remove punctuation

def remove_tags(text):
    # define the pattern of html tags to remove
    pattern =re.sub(r'<.*?>',"",text)
    # use regex to substitute the pattern with an empty string
    return pattern

untag_text =  remove_tags(unpunc_text)  # level 1-3 of preprocessing: remove html tag

import re

def remove_emojis(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F" # emoticons
                           u"\U0001F300-\U0001F5FF" # symbols & pictographs
                           u"\U0001F680-\U0001F6FF" # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF" # flags (iOS)
                           u"\U00002500-\U00002BEF" # chinese char
                           u"\U00002702-\U000027B0"
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           u"\U0001f926-\U0001f937"
                           u"\U00010000-\U0010ffff"
                           u"\u2640-\u2642"
                           u"\u2600-\u2B55"
                           u"\u200d"
                           u"\u23cf"
                           u"\u23e9"
                           u"\u231a"
                           u"\ufe0f" # dingbats
                           u"\u3030"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

unemoji_text = remove_emojis(untag_text) # level 1-3 of preprocessing: remove emojis

from hazm import normalizer

# level 1-3 of preprocessing: Normalization
normalizer = Normalizer()
normalized_text = normalizer.normalize(unemoji_text)

def replace_num_url(text):

  text = re.sub(r'\d+','<NUM>', text) # Num token
  text = re.sub(r'http\S+', '<URL>', text) # URL token

  return text

new_text = replace_num_url(normalized_text) # level 1-4 of preprocessing

# level 2-1 of language model
from collections import Counter

def num_ngrams(text): # Calculate number of ngrams

    words = text.split()  # Split text into words

    # Create unigrams
    unigrams = words

    # Create bigrams
    bigrams = [words[i] + " " + words[i+1] for i in range(len(words)-1)]

    # Create trigrams
    trigrams = [words[i] + " " + words[i+1] + " " + words[i+2] for i in range(len(words)-2)]

    # The number of repetitions each ngram
    unigram_counts = Counter(unigrams)
    bigram_counts = Counter(bigrams)
    trigram_counts = Counter(trigrams)

    return unigram_counts, bigram_counts, trigram_counts

def show_ngrams(text): # Show ngrams list

    words = text.split()  # Split text into words

    # Create unigrams
    unigrams = words

    # Create bigrams
    bigrams = [words[i] + " " + words[i+1] for i in range(len(words)-1)]

    # Create trigrams
    trigrams = [words[i] + " " + words[i+1] + " " + words[i+2] for i in range(len(words)-2)]


    print("Unigrams: ", unigrams, "\n")
    print("Bigrams: ", bigrams, "\n")
    print("Trigrams: ", trigrams, "\n")



def print_most_common(ngram_counts):
    for ngram, count in ngram_counts.most_common(10):
        print(ngram, count)

unigram_counts, bigram_counts, trigram_counts = num_ngrams(new_text)
#show_ngrams(new_text)



print("10 most common Unigrams:")
print_most_common(unigram_counts)

print("10 most common Bigrams:")
print_most_common(bigram_counts)

print("10 most common Trigrams:")
print_most_common(trigram_counts)

# level 2-2 of language model
import nltk
from nltk.probability import LaplaceProbDist, SimpleGoodTuringProbDist


def create_ngrams(text):

    words = text.split()  # Split text into words

    # Create unigrams
    unigrams = words

    # Create bigrams
    bigrams = [words[i] + " " + words[i+1] for i in range(len(words)-1)]

    # Create trigrams
    trigrams = [words[i] + " " + words[i+1] + " " + words[i+2] for i in range(len(words)-2)]

    return unigrams, bigrams, trigrams

def calculate_probability(ngram, n, corpus):
    if n == 1:
        # Laplace smoothing for unigrams
        freq_dist = nltk.FreqDist(corpus)
        prob_dist = LaplaceProbDist(freq_dist)
        return prob_dist.prob(ngram)
    else:
        # Turing-Good smoothing for other n-grams
        ngrams = list(nltk.ngrams(corpus, n))
        freq_dist = nltk.FreqDist(ngrams)
        prob_dist = SimpleGoodTuringProbDist(freq_dist)
        return prob_dist.prob(ngram)

# The Laplace smoothing method is not good for n-grams because as the size of n increases, the number of n-grams becomes very large,
# and as a result, the number of repetitions of each n-gram decreases. In other words, as n increases,
# the available data for smoothing becomes less and this makes Laplace smoothing not give better results than other methods.
# For larger n-grams, the Turing-Good smoothing method is usually used,
# which estimates the number of repetitions of each n-gram according to the number of different n-grams and then uses smoothing.

#level 2-3 of language model
import math

def perplexity(sentence, ngram_model):
    unigrams, bigrams, trigrams = create_ngrams(sentence)
    words = sentence.split()
    probability_sum = 0
    n = len(words)

    if ngram_model == "unigram":
        for word in words:
            probability = unigrams.count(word)+1 / len(unigrams)+len(set(unigrams))
            probability_sum += math.log2(probability)
        perplexity = 2 ** (-probability_sum / n)

    elif ngram_model == "bigram":
        for i in range(1, n):
            bigram = words[i-1] + " " + words[i]
            if bigram in bigrams:
                probability = bigrams.count(bigram)+1 / unigrams.count(words[i-1])+len(set(bigrams))
                probability_sum += math.log2(probability)
            else:
                probability_sum += math.log2(1/len(unigrams))
        perplexity = 2 ** (-probability_sum / n)

    elif ngram_model == "trigram":
        for i in range(2, n):
            trigram = words[i-2] + " " + words[i-1] + " " + words[i]
            if trigram in trigrams:
                probability = trigrams.count(trigram)+1 / bigrams.count(words[i-2] + " " + words[i-1])+len(set(trigrams))
                probability_sum += math.log2(probability)
            else:
                bigram = words[i-2] + " " + words[i-1]
                if bigram in bigrams:
                    probability = bigrams.count(bigram)+1 / unigrams.count(words[i-2])+len(set(bigrams))
                    probability_sum += math.log2(probability)
                else:
                    probability_sum += math.log2(1/len(unigrams))+len(set(bigrams))
        perplexity = 2 ** (-probability_sum / n)

    return perplexity


sentences = ['بوی تند ولی خوشبو داره', 'بلوتوثش کار نمی کنه حالا تا بدستم رسیده باید برشگردونم',
             'لطفا کالای مورد نظر رو در پیشنهاد ویژه قرار بدید',
             'بلندگوهاش بیس بالا و صدای زیادی بمی داره که بعد از مدتی باعث خسته شدن مغز آدم میشه']

# Show perplexity of each sentence
for sentence in sentences:
    unigram_perplexity = perplexity(sentence, "unigram")
    bigram_perplexity = perplexity(sentence, "bigram")
    trigram_perplexity = perplexity(sentence, "trigram")

    print('Unigram Perplexity for sentence "', sentence, '": ', unigram_perplexity)
    print('Bigram Perplexity for sentence "', sentence, '": ', bigram_perplexity)
    print('Trigram Perplexity for sentence "', sentence, '": ', trigram_perplexity, "\n")

#level 2-4 of language model
import random

def suggest_next_word(model, sentence):
    # Get ngrams from sentence
    unigrams, bigrams, trigrams = create_ngrams(sentence)

    # Determine which ngram to use for prediction
    if model == 'unigram':
        ngram = create_ngrams(new_text)[0]
    elif model == 'bigram':
        ngram = create_ngrams(new_text)[1]
    elif model == 'trigram':
        ngram = create_ngrams(new_text)[2]
    else:
        raise ValueError('Invalid model specified. Model must be unigram, bigram, or trigram.')

    # Choose a random word from the ngram as the next predicted word
    next_word = random.choice(ngram)

    return next_word

def generate_sentence(model, text):
    # Split input text into words
    words = text.split()

    # Generate new words until the sentence reaches 15 words
    while len(words) < 15:
        # Join words into a sentence
        sentence = ''.join(words)

        # Predict the next word based on the model and current sentence
        next_word = suggest_next_word(model, sentence)

        # Append the predicted word to the list of words
        words.append(next_word)

    # Join final list of words into a sentence and return it
    return ' '.join(words)


sentence_list = ['صرفه جویی در پودر ماشین','یکی از چراغهای وضعیت','گوشی سامسونگ','رنگ قرمز کفش','یک تن ماهی خوب']
model_list = ['unigram','bigram','trigram']


for i in range(len(sentence_list)):
  for j in range(len(model_list)):
    print(model_list[j],"model: ",generate_sentence(model_list[j], sentence_list[i]))
  print("\n")

#level 2-5 of language model

sentence_list = ['صرفه جویی در پودر ماشین','یکی از چراغهای وضعیت','گوشی سامسونگ','رنگ قرمز کفش','یک تن ماهی خوب']
model_list = ['unigram','bigram','trigram']

for sentence in sentence_list:
    for model in model_list:
      result = perplexity(sentence, model)
      print(model, " ", 'Perplexity for sentence "', sentence, '": ', result)

    print("\n")

#level 3-1 of tagging
import hazm

preprocessed_text = new_text

# Initialize a tagger
tagger = hazm.POSTagger(model='pos_tagger.model')

# Tokenize the preprocessed text
tokens = hazm.word_tokenize(preprocessed_text)

# Apply POS tagging on tokens
tagged_tokens = tagger.tag(tokens)

# Display the tag of each token
for token, tag in tagged_tokens:
    print(f'Token: {token}\t Tag: {tag}')

#level 3-2 of tagging
from hazm import POSTagger
from collections import Counter


# Count the occurrences of each POS tag
pos_counts = Counter(tag for word, tag in tagged_tokens)

# Report the number of occurrences of each POS tag
for tag, count in pos_counts.items():
    print(f"{tag}: {count}")

#level 3-3 of tagging
from hazm import Normalizer, sent_tokenize, word_tokenize, POSTagger
from collections import Counter


preprocessed_data = [preprocessed_text]

# Normalize, tokenize, and tag the sentences
normalizer = Normalizer()
tagger = POSTagger(model='pos_tagger.model')
tagged_nouns = []

for sentence in preprocessed_data:
    normalized_sentence = normalizer.normalize(sentence)
    tokens = word_tokenize(normalized_sentence)
    tagged_sent = tagger.tag(tokens)
    tagged_nouns.extend([word for (word, tag) in tagged_sent if tag.startswith('N')])

# Count the occurrences of each noun
noun_counts = Counter(tagged_nouns)

# Get the most repeated 15 nouns
most_repeated_nouns = noun_counts.most_common(15)

# Print the nouns along with their frequencies
for noun, count in most_repeated_nouns:
    print(noun, count)